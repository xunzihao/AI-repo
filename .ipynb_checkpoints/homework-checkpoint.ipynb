{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.080492Z",
     "start_time": "2024-06-04T13:26:32.078464Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import BertTokenizer"
   ],
   "execution_count": 380,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1.数据准备 \n",
    "读入原始数据。原始数据按照序列标注任务的格式组织，我们需要将其处理成文本序列、实体标注序列和情感标注序列，并转换为Tensor构成的InputFeature，符合BERT对输入格式的要求。\n",
    "（1）读入原始数据\n",
    "（2）将数据包装为InputExample\n",
    "（3）Tokenize，并将不同长度的输入补齐，将InputExample提取特征为InputFeature，并转化为Tensor，可以作为神经网络的输入。"
   ],
   "id": "bae6f700c1d45261"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1.1 读入原始数据，转换成文本序列和标注序列\n",
    "转换方式与NER任务相似，这里的标注序列有两个，一个是实体的标注序列，通过O、B-ASP、I-ASP标注出文本中的实体，\n",
    "另一个是情感的标注序列，通过0、-1、2标注出对应实体的情感倾向性。\n",
    "如果一个token不是实体（实体标注序列中对应位置为O），在情感标注序列中它的标签为0。"
   ],
   "id": "c7f7f8600b43cf24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.161443Z",
     "start_time": "2024-06-04T13:26:32.152765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, encoding='utf8')\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tag = []\n",
    "    polarity = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, tag, polarity))\n",
    "                sentence = []\n",
    "                tag = []\n",
    "                polarity = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        if len(splits) != 3:\n",
    "            print('warning! detected error line(s) in input file:{}'.format(line))\n",
    "        sentence.append(splits[0])\n",
    "        tag.append(splits[-2])\n",
    "        polarity.append(int(splits[-1][:-1]))\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, tag, polarity))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = readfile(\"./datasets/notebook/notebook.atepc.train.dat\")\n",
    "test_data = readfile(\"./datasets/notebook/notebook.atepc.test.dat\")\n",
    "\n",
    "print(\"训练集数量：%d 测试集数量：%d\" % (len(train_data), len(test_data)))\n",
    "print(\"实例：\")\n",
    "print(train_data[0])"
   ],
   "id": "de0e229bfd5802d2",
   "execution_count": 381,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1.2 将读入的数据包装为InputExample的形式\n",
    "这种写法是沿用BERT的代码。在Example中的属性包括："
   ],
   "id": "af48bbb8a58fbf5a"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "guid：生成的唯一id\n",
    "text_a: 输入的句子\n",
    "text_b: 句子中的实体\n",
    "sentence_label: 句子的标注，即text_a的标注，其中实体对应标注为B-ASP和I-ASP，非实体的token标注为O\n",
    "aspect_label: 实体标注，对text_b的实体序列标注\n",
    "polarity: 情感标注，text_a中每个token的情感倾向性标注"
   ],
   "id": "f7ac84fcc6771484"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.167354Z",
     "start_time": "2024-06-04T13:26:32.162346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, text_a, text_b=None, sentence_label=None, aspect_label=None, polarity=None):\n",
    "        self.guid = guid  # 输入数据的id\n",
    "        self.text_a = text_a  # 输入的句子\n",
    "        self.text_b = text_b  # 句子中的aspect\n",
    "        self.sentence_label = sentence_label  # 句子标注\n",
    "        self.aspect_label = aspect_label  # text_b的标注\n",
    "        self.polarity = polarity  # 情感倾向\n",
    "\n",
    "\n",
    "def create_example(lines, set_type):\n",
    "    examples = []\n",
    "    for i, (sentence, tag, polarity) in enumerate(lines):\n",
    "        aspect = []\n",
    "        aspect_tag = []\n",
    "        aspect_polarity = [-1]\n",
    "        for w, t, p in zip(sentence, tag, polarity):\n",
    "            if p != -1:\n",
    "                aspect.append(w)\n",
    "                aspect_tag.append(t)\n",
    "                aspect_polarity.append(-1)\n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        text_a = sentence\n",
    "        text_b = aspect\n",
    "        polarity.extend(aspect_polarity)\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, sentence_label=tag,\n",
    "                         aspect_label=aspect_tag, polarity=polarity))\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_examples = create_example(train_data, \"train\")\n",
    "test_examples = create_example(test_data, \"test\")\n",
    "print(train_examples[0].guid)\n",
    "print(train_examples[0].text_a)\n",
    "print(train_examples[0].text_b)\n",
    "print(train_examples[0].sentence_label)\n",
    "print(train_examples[0].aspect_label)\n",
    "print(train_examples[0].polarity)"
   ],
   "id": "b6ad66c1a1b63e1d",
   "execution_count": 382,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1.3.1 一些设置\n",
    "MAX_SEQUENCE_LENGTH： 最大句子长度\n",
    "LABEL_LIST：实体标签\n",
    "PRETRAINED_BERT_MODEL：使用的预训练BERT模型\n",
    "NUM_LABELS：实体标签数，这里多出来的一个标签是为padding出来的位置准备的"
   ],
   "id": "8315d1a3528629d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.170099Z",
     "start_time": "2024-06-04T13:26:32.168389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_SEQUENCE_LENGTH = 80\n",
    "LABEL_LIST = [\"O\", \"B-ASP\", \"I-ASP\", \"[CLS]\", \"[SEP]\"]\n",
    "PRETRAINED_BERT_MODEL = \"bert-base-chinese\"\n",
    "NUM_LABELS = len(LABEL_LIST) + 1"
   ],
   "id": "be70144259316fc7",
   "execution_count": 383,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1.3.2 从预训练的模型中加载tokenizer\n",
    "这里会自动从huggingface的网站上下载预训练模型的词典，tokenizer会完成WordPiece的分词工作。"
   ],
   "id": "bbc4b5de7c50910e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.468680Z",
     "start_time": "2024-06-04T13:26:32.172697Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_MODEL, do_lower_case=True)",
   "id": "82abcc65604ccfec",
   "execution_count": 384,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.3.3 将原来数据中的情感标签对齐",
   "id": "ac6181a327f61c45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.473615Z",
     "start_time": "2024-06-04T13:26:32.470034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_polarity(examples):\n",
    "    for i in range(len(examples)):\n",
    "        polarities = []\n",
    "        for polarity in examples[i].polarity:\n",
    "            if polarity == 2:\n",
    "                polarities.append(1)\n",
    "            else:\n",
    "                polarities.append(polarity)\n",
    "        examples[i].polarity = polarities\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_examples = convert_polarity(train_examples)\n",
    "test_examples = convert_polarity(test_examples)"
   ],
   "id": "b5c7cdf0a9eab077",
   "execution_count": 385,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1.3.4 将Example转化为输入特征\n",
    "InputFeatures类作为神经网络的输入，在转换时将所有属性padding到MAX_SEQUENCE_LENGTH。按照BERT的输入格式，text_a和text_b之间通过'[SEP]'符号连接\n",
    "其中的属性包括：\n",
    "input_ids_spc: 编码后的输入文本\n",
    "input_mask: BERT模型中的mask，为1的位置表示真实词，为0表示padding的占位符\n",
    "segment_ids: BERT模型中的segment，为0表示text_a\n",
    "label_id: 实体标注序列\n",
    "label_mask: 实体标注序列的mask，表示哪些是真实标注，为0的位置表示padding占位符的标注\n",
    "polarities: 情感标注序列"
   ],
   "id": "a4764fdccfece9b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.476690Z",
     "start_time": "2024-06-04T13:26:32.474456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids_spc, input_mask, segment_ids, label_id,\n",
    "                 polarities=None, valid_ids=None, label_mask=None):\n",
    "        self.input_ids_spc = input_ids_spc\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\n",
    "        self.polarities = polarities"
   ],
   "id": "39329f9143808ef6",
   "execution_count": 386,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.485091Z",
     "start_time": "2024-06-04T13:26:32.477956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
    "\n",
    "    features = []\n",
    "    for example in tqdm.tqdm(examples):\n",
    "        text_spc_tokens = example.text_a\n",
    "        aspect_tokens = example.text_b\n",
    "        sentence_label = example.sentence_label\n",
    "        aspect_label = example.aspect_label\n",
    "        polaritiylist = example.polarity\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        polarities = []\n",
    "        valid = []\n",
    "        label_mask = []\n",
    "        text_spc_tokens.extend(['[SEP]'])\n",
    "        text_spc_tokens.extend(aspect_tokens)  # 将输入文本（text_a）和识别出来的实体(text_b)连接起来\n",
    "        enum_tokens = text_spc_tokens\n",
    "        sentence_label.extend(['[SEP]'])\n",
    "        # sentence_label.extend(['O'])\n",
    "        sentence_label.extend(aspect_label)\n",
    "        label_lists = sentence_label\n",
    "        for i, word in enumerate(enum_tokens):  # 为文本和实体生成标签序列\n",
    "            token = tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            label_1 = label_lists[i]\n",
    "            polarity_1 = polaritiylist[i]\n",
    "            for m in range(len(token)):  # 一个词中不同字，只在首字上标注\n",
    "                if m == 0:\n",
    "                    labels.append(label_1)\n",
    "                    polarities.append(polarity_1)\n",
    "                    valid.append(1)\n",
    "                    label_mask.append(1)\n",
    "                else:\n",
    "                    valid.append(0)\n",
    "        if len(tokens) >= max_seq_length - 1:\n",
    "            tokens = tokens[0:(max_seq_length - 2)]\n",
    "            polarities = polarities[0:(max_seq_length - 2)]\n",
    "            labels = labels[0:(max_seq_length - 2)]\n",
    "            valid = valid[0:(max_seq_length - 2)]\n",
    "            label_mask = label_mask[0:(max_seq_length - 2)]\n",
    "            ntokens = []\n",
    "        segment_ids = []\n",
    "        label_ids = []\n",
    "        ntokens = []\n",
    "        ntokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.insert(0, 1)\n",
    "        label_mask.insert(0, 1)\n",
    "        label_ids.append(label_map[\"[CLS]\"])\n",
    "        for i, token in enumerate(tokens):\n",
    "            ntokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        ntokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.append(1)\n",
    "        label_mask.append(1)\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "        input_ids_spc = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        input_mask = [1] * len(input_ids_spc)\n",
    "        label_mask = [1] * len(label_ids)\n",
    "        # 将各属性补齐\n",
    "        while len(input_ids_spc) < max_seq_length:\n",
    "            input_ids_spc.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(0)\n",
    "            valid.append(1)\n",
    "            label_mask.append(0)\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(0)\n",
    "        while len(polarities) < max_seq_length:\n",
    "            polarities.append(-1)\n",
    "            assert len(input_ids_spc) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids_spc=input_ids_spc,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_ids,\n",
    "                          polarities=polarities,\n",
    "                          valid_ids=valid,\n",
    "                          label_mask=label_mask))\n",
    "    return features"
   ],
   "id": "c0e625efd0eb451d",
   "execution_count": 387,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.631162Z",
     "start_time": "2024-06-04T13:26:32.485777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_features = convert_examples_to_features(train_examples, LABEL_LIST, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_features(test_examples, LABEL_LIST, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "print(train_features[0].input_ids_spc)\n",
    "print(train_features[0].input_mask)\n",
    "print(train_features[0].segment_ids)\n",
    "print(train_features[0].label_id)\n",
    "print(train_features[0].valid_ids)\n",
    "print(train_features[0].label_mask)\n",
    "print(train_features[0].polarities)"
   ],
   "id": "a7bdcdca5eb043b1",
   "execution_count": 388,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "2.构造模型\n",
    "在本教程中我们使用pytorch深度学习框架和基于Pytorch实现的Pytorch_transformer模块，这一模块实现了BERT为代表的多种transformer模型，并将预训练语言模型集成到模块中，通过一行代码就可以方便地下载并加载预训练模型。\n",
    "（1）加载预训练的BERT模型\n",
    "（2）定义网络结构\n",
    "（3）定义优化器\n",
    "（4）定义输入"
   ],
   "id": "126c11466aeb7417"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:32.633869Z",
     "start_time": "2024-06-04T13:26:32.632145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 32 \n",
    "# DEVICE = \"cpu\"#\n",
    "# DEVICE = \"cuda:0\"\n",
    "DEVICE = \"mps\""
   ],
   "id": "59ad4309bfd677d3",
   "execution_count": 389,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "2.1 加载预训练的BERT模型¶这里会自动从huggingface的网站上下载预训练的BERT模型，在下载之后需要重新设置类别数量\n",
    "这里会自动从huggingface的网站上下载预训练的BERT模型，在下载之后需要重新设置类别数量"
   ],
   "id": "3b59ee241dc55537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:33.404559Z",
     "start_time": "2024-06-04T13:26:32.634537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_base_model = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n",
    "bert_base_model.config.num_labels = NUM_LABELS"
   ],
   "id": "514ceec77a1ba731",
   "execution_count": 390,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "2.2.1 定义自注意力机制\n",
    "先在BERT的基础上实现一个新的自注意力机制"
   ],
   "id": "4b73ab1231ca9dd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:33.408331Z",
     "start_time": "2024-06-04T13:26:33.405503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
    "\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.config = config\n",
    "        self.SA = BertSelfAttention(config)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_vec = np.zeros((inputs.size(0), 1, 1, MAX_SEQUENCE_LENGTH))\n",
    "        zero_tensor = torch.tensor(zero_vec).float().to(DEVICE)\n",
    "        SA_out = self.SA(inputs, zero_tensor)\n",
    "        return self.tanh(SA_out[0])"
   ],
   "id": "8ec63cc3f2710268",
   "execution_count": 391,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "2.2.2 模型网络结构\n",
    "在Pytorch中，可以通过定义一个Module类的子类，并覆盖其forward方法。\n",
    "pytorch_transformer中的BertForTokenClassification类是Module的子类，\n",
    "我们继承BertForTokenClassification类，覆盖里面的forward方法。forward方法定义了神经网络前向传播的过程。"
   ],
   "id": "8bad860f6cc41750"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:36.051681Z",
     "start_time": "2024-06-04T13:26:33.412189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "\n",
    "class Model(BertForTokenClassification):\n",
    "    def __init__(self, bert_base_model):\n",
    "        config = bert_base_model.config\n",
    "        super(Model, self).__init__(config=config)\n",
    "        self.bert_for_global_context = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n",
    "        self.bert_for_local_context = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.dense = torch.nn.Linear(768, 3)\n",
    "        # self.bert_global_focus = self.bert_for_global_context\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.SA1 = SelfAttention(config)\n",
    "        self.SA2 = SelfAttention(config)\n",
    "        self.linear_double = torch.nn.Linear(768 * 2, 768)\n",
    "        self.linear_triple = torch.nn.Linear(768 * 3, 768)\n",
    "\n",
    "    def get_ids_for_local_context_extractor(self, text_indices):\n",
    "        text_ids = text_indices.detach().cpu().numpy()\n",
    "        for text_i in range(len(text_ids)):\n",
    "            sep_index = np.argmax((text_ids[text_i] == 102))\n",
    "            text_ids[text_i][sep_index + 1:] = 0\n",
    "        return torch.tensor(text_ids).to(DEVICE)\n",
    "\n",
    "    def get_batch_token_labels_bert_base_indices(self, labels):\n",
    "        if labels is None:\n",
    "            return\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        for text_i in range(len(labels)):\n",
    "            sep_index = np.argmax((labels[text_i] == 5))\n",
    "            labels[text_i][sep_index + 1:] = 0\n",
    "        return torch.tensor(labels).to(DEVICE)\n",
    "\n",
    "    def get_batch_polarities(self, b_polarities):\n",
    "        b_polarities = b_polarities.detach().cpu().numpy()\n",
    "        shape = b_polarities.shape\n",
    "        polarities = np.zeros((shape[0]))\n",
    "        i = 0\n",
    "        for polarity in b_polarities:\n",
    "            polarity_idx = np.flatnonzero(polarity + 1)\n",
    "            polarities[i] = polarity[polarity_idx[0]]\n",
    "            i += 1\n",
    "        polarities = torch.from_numpy(polarities).long().to(DEVICE)\n",
    "        return polarities\n",
    "\n",
    "    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarities=None, valid_ids=None, attention_mask_label=None):\n",
    "        input_ids_spc = self.get_ids_for_local_context_extractor(input_ids_spc)\n",
    "        labels = self.get_batch_token_labels_bert_base_indices(labels)\n",
    "        global_context_out = self.bert_for_global_context(input_ids_spc, attention_mask=attention_mask)[0]\n",
    "        polarity_labels = self.get_batch_polarities(polarities)\n",
    "        batch_size, max_len, feat_dim = global_context_out.shape\n",
    "        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32).to(DEVICE)\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(max_len):\n",
    "                if valid_ids[i][j].item() == 1:\n",
    "                    jj += 1\n",
    "                    global_valid_output[i][jj] = global_context_out[i][j]\n",
    "        global_context_out = self.dropout(global_valid_output)\n",
    "        ate_logits = self.classifier(global_context_out)\n",
    "        pooled_out = self.pooler(global_context_out)\n",
    "        pooled_out = self.dropout(pooled_out)\n",
    "        apc_logits = self.dense(pooled_out)\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "            loss_sen = torch.nn.CrossEntropyLoss()\n",
    "            loss_ate = loss_fct(ate_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss_apc = loss_sen(apc_logits, polarity_labels)\n",
    "            return loss_ate, loss_apc\n",
    "        else:\n",
    "            return ate_logits, apc_logits\n",
    "\n",
    "model = Model(bert_base_model)\n"
   ],
   "id": "9de3cd9308562bb0",
   "execution_count": 392,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.2.3 将模型加载到对应的计算设备上¶与tensorflow不同，Pytorch需要我们通过函数调用完成模型加载到设备上的过程。",
   "id": "b372da6f777a210a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:37.165270Z",
     "start_time": "2024-06-04T13:26:36.053953Z"
    }
   },
   "cell_type": "code",
   "source": "_ = model.to(DEVICE)",
   "id": "c46cf9b143131b2c",
   "execution_count": 393,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "2.3 设置优化器\n",
    "与其他教程相似，在训练中我们使用学习率衰减的策略"
   ],
   "id": "87a49abbe3bf4be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:37.171009Z",
     "start_time": "2024-06-04T13:26:37.165955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "param_optimizer = list(model.named_parameters())  # 模型中的所有参数\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.00001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.00001}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, weight_decay=0.00001)"
   ],
   "id": "ad49195d52c46f46",
   "execution_count": 394,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "2.4 设置输入\n",
    "构造DataLoader，首先将InputFeature中的特征转化为pytorch中的Tensor，然后生成TensorDataset和SequentialSampler，再将dataset和sampler结合成为DataLoader，为之后模型训练和测试提供数据。\n",
    "DataLoader是一个可迭代对象，在训练和测试时可以每次返回一个batch的数据，并且可以利用多进程进行加速。"
   ],
   "id": "9d47ac0c1e77f160"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.4.1 设置训练的输入",
   "id": "b1e1d0acc89da28e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:37.184562Z",
     "start_time": "2024-06-04T13:26:37.171729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
    "\n",
    "all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\n",
    "all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\n",
    "all_polarities = torch.tensor([f.polarities for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_spc_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ],
   "id": "7011d75ff85a97d4",
   "execution_count": 395,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.4.2 设置测试的输入",
   "id": "16cbcfe3767bd86a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:37.191177Z",
     "start_time": "2024-06-04T13:26:37.185274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_polarities = torch.tensor([f.polarities for f in test_features], dtype=torch.long)\n",
    "all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)\n",
    "all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_spc_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids)\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=BATCH_SIZE)"
   ],
   "id": "54a74fe44efe551a",
   "execution_count": 396,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "3.训练模型\n",
    "（1）设置模型训练过程中的超参数，初始化日志\n",
    "（2）定义评估函数\n",
    "（3）模型训练并监督训练过程\n",
    "（4）保存模型"
   ],
   "id": "a405d83aea19350a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.1 设置训练过程中的超参数，并利用logging模块输出训练过程的日志",
   "id": "9aad770eadfdd7b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:37.194209Z",
     "start_time": "2024-06-04T13:26:37.192100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "EPOCH = 5  # 共计算5个epoch\n",
    "EVAL_STEP = 10  # 每10个step执行一个评估\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ],
   "id": "b8c643b7d26768a4",
   "execution_count": 397,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "3.2 定义评估函数\n",
    "这里我们用scikit-learn中计算的F1值作为评价指标。"
   ],
   "id": "d85ebe092e175758"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:26:37.200526Z",
     "start_time": "2024-06-04T13:26:37.194860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(dataloader, label_list):\n",
    "    apc_result = {'max_apc_test_acc': 0, 'max_apc_test_f1': 0}\n",
    "    ate_result = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    n_test_correct, n_test_total = 0, 0\n",
    "    test_apc_logits_all, test_polarities_all = None, None\n",
    "    model.eval()  # 将网络设置为评估的状态\n",
    "    label_map = {i: label for i, label in enumerate(label_list, 1)}\n",
    "    for input_ids_spc, input_mask, segment_ids, label_ids, polarities, valid_ids, l_mask in dataloader:\n",
    "        input_ids_spc = input_ids_spc.to(DEVICE)\n",
    "        input_mask = input_mask.to(DEVICE)\n",
    "        segment_ids = segment_ids.to(DEVICE)\n",
    "        valid_ids = valid_ids.to(DEVICE)\n",
    "        label_ids = label_ids.to(DEVICE)\n",
    "        polarities = polarities.to(DEVICE)\n",
    "        l_mask = l_mask.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            ate_logits, apc_logits = model(\n",
    "                input_ids_spc, segment_ids, input_mask, \n",
    "                valid_ids=valid_ids, polarities=polarities, attention_mask_label=l_mask)\n",
    "        polarities = model.get_batch_polarities(polarities)\n",
    "        n_test_correct += (torch.argmax(apc_logits, -1) == polarities).sum().item()\n",
    "        n_test_total += len(polarities)\n",
    "        n_test_total += len(polarities)\n",
    "        if test_polarities_all is None:\n",
    "            test_polarities_all = polarities\n",
    "            test_apc_logits_all = apc_logits\n",
    "        else:\n",
    "            test_polarities_all = torch.cat((test_polarities_all, polarities), dim=0)\n",
    "            test_apc_logits_all = torch.cat((test_apc_logits_all, apc_logits), dim=0)\n",
    "            label_ids = model.get_batch_token_labels_bert_base_indices(label_ids)\n",
    "        ate_logits = torch.argmax(F.log_softmax(ate_logits, dim=2), dim=2)\n",
    "        ate_logits = ate_logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        input_mask = input_mask.to('cpu').numpy()\n",
    "        for i, label in enumerate(label_ids):\n",
    "            temp_1 = []\n",
    "            temp_2 = []\n",
    "            for j, m in enumerate(label):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                elif label_ids[i][j] == len(label_list):\n",
    "                    y_true += temp_1\n",
    "                    y_pred += temp_2\n",
    "                    break\n",
    "                else:\n",
    "                    temp_1.append(label_map.get(label_ids[i][j], 'O'))\n",
    "                    temp_2.append(label_map.get(ate_logits[i][j], 'O'))\n",
    "                    test_acc = n_test_correct / n_test_total\n",
    "    test_f1 = f1_score(torch.argmax(test_apc_logits_all, -1).cpu(), test_polarities_all.cpu(),                                   labels=[0, 1], average='macro')\n",
    "    test_acc = round(test_acc * 100, 2)\n",
    "    test_f1 = round(test_f1 * 100, 2)\n",
    "    apc_result = {'max_apc_test_acc': test_acc, 'max_apc_test_f1': test_f1}    \n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    tmps = report.split()\n",
    "    ate_result = round(float(tmps[7]) * 100, 2)\n",
    "    return apc_result, ate_result"
   ],
   "id": "2b38f3689fbda71",
   "execution_count": 398,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "3.3 训练模型\n",
    "在每个EPOCH中，每次输入一个batch的数据，计算损失和损失反向传播完成一个step的训练。每经过EVAL_STEP个step做一次评估，并记录下训练过程中最好的评价指标\n",
    "TIPS: 在前向传播之前要把模型的模式设为训练模式（第10行），这会把网络中的Dropout和BatchNormilzation关掉。"
   ],
   "id": "e0bae439a0164716"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:27:53.541870Z",
     "start_time": "2024-06-04T13:26:37.201143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_apc_test_acc = 0\n",
    "max_apc_test_f1 = 0\n",
    "max_ate_test_f1 = 0\n",
    "global_step = 0\n",
    "print(range(EPOCH))\n",
    "for epoch in range(EPOCH):\n",
    "    # 每个epoch\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 一个step\n",
    "        model.train()  # 将网络设置为train的模式\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids_spc, input_mask, segment_ids, label_ids, polarities, valid_ids, l_mask = batch  # 取一个batch的数据\n",
    "\n",
    "        loss_ate, loss_apc = model(\n",
    "            input_ids_spc, segment_ids, input_mask, label_ids, polarities, valid_ids, l_mask)  # 前向传播，计算损失\n",
    "        loss = loss_ate + loss_apc\n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "        nb_tr_examples += input_ids_spc.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        if global_step % EVAL_STEP == 0:  # 评估\n",
    "            apc_result, ate_result = evaluate(eval_dataloader, LABEL_LIST)\n",
    "            if apc_result['max_apc_test_acc'] > max_apc_test_acc:\n",
    "                max_apc_test_acc = apc_result['max_apc_test_acc']\n",
    "            if apc_result['max_apc_test_f1'] > max_apc_test_f1:\n",
    "                max_apc_test_f1 = apc_result['max_apc_test_f1']\n",
    "            if ate_result > max_ate_test_f1:\n",
    "                max_ate_test_f1 = ate_result\n",
    "            current_apc_test_acc = apc_result['max_apc_test_acc']\n",
    "            current_apc_test_f1 = apc_result['max_apc_test_f1']\n",
    "            current_ate_test_f1 = round(ate_result, 2)\n",
    "    logger.info('Epoch %s' % epoch)\n",
    "    logger.info(f'APC_test_acc: {current_apc_test_acc}(max: {max_apc_test_acc})  '\n",
    "                        f'APC_test_f1: {current_apc_test_f1}(max: {max_apc_test_f1})')\n",
    "    logger.info(f'ATE_test_f1: {current_ate_test_f1}(max:{max_ate_test_f1})')"
   ],
   "id": "1ca2d8538e043ce6",
   "execution_count": 399,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.4 保存模型",
   "id": "ba6afff717c6f199"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:28:11.721422Z",
     "start_time": "2024-06-04T13:28:10.754398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "SAVE_PATH = \"./temp\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "label_map = {i : label for i, label in enumerate(LABEL_LIST,1)}\n",
    "model_config = {\n",
    "    \"bert_model\": PRETRAINED_BERT_MODEL,\n",
    "    \"do_lower\": True,\n",
    "    \"max_seq_length\": MAX_SEQUENCE_LENGTH,\n",
    "    \"num_labels\": len(LABEL_LIST)+1,\n",
    "    \"label_map\": label_map\n",
    "}\n",
    "json.dump(model_config, open(os.path.join(SAVE_PATH, \"config.json\"), \"w\"))"
   ],
   "id": "d0419f1b97718788",
   "execution_count": 401,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T13:27:54.459808Z",
     "start_time": "2024-06-04T13:27:54.458594Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "27ccec66b1e9f811",
   "execution_count": 400,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
