{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:00.529Z",
     "start_time": "2024-06-09T05:29:00.526905Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import BertTokenizer"
   ],
   "outputs": [],
   "execution_count": 623
  },
  {
   "cell_type": "raw",
   "id": "bae6f700c1d45261",
   "metadata": {},
   "source": [
    "1.数据准备 \n",
    "读入原始数据。原始数据按照序列标注任务的格式组织，我们需要将其处理成文本序列、实体标注序列和情感标注序列，并转换为Tensor构成的InputFeature，符合BERT对输入格式的要求。\n",
    "（1）读入原始数据\n",
    "（2）将数据包装为InputExample\n",
    "（3）Tokenize，并将不同长度的输入补齐，将InputExample提取特征为InputFeature，并转化为Tensor，可以作为神经网络的输入。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7f7f8600b43cf24",
   "metadata": {},
   "source": [
    "1.1 读入原始数据，转换成文本序列和标注序列\n",
    "转换方式与NER任务相似，这里的标注序列有两个，一个是实体的标注序列，通过O、B-ASP、I-ASP标注出文本中的实体，\n",
    "另一个是情感的标注序列，通过0、-1、2标注出对应实体的情感倾向性。\n",
    "如果一个token不是实体（实体标注序列中对应位置为O），在情感标注序列中它的标签为0。"
   ]
  },
  {
   "cell_type": "code",
   "id": "de0e229bfd5802d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:00.679517Z",
     "start_time": "2024-06-09T05:29:00.670354Z"
    }
   },
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, encoding='utf8')\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tag = []\n",
    "    polarity = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, tag, polarity))\n",
    "                sentence = []\n",
    "                tag = []\n",
    "                polarity = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        if len(splits) != 3:\n",
    "            print('warning! detected error line(s) in input file:{}'.format(line))\n",
    "        sentence.append(splits[0])\n",
    "        tag.append(splits[-2])\n",
    "        polarity.append(int(splits[-1][:-1]))\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, tag, polarity))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = readfile(\"./datasets/notebook/notebook.atepc.train.dat\")\n",
    "test_data = readfile(\"./datasets/notebook/notebook.atepc.test.dat\")\n",
    "\n",
    "print(\"训练集数量：%d 测试集数量：%d\" % (len(train_data), len(test_data)))\n",
    "print(\"实例：\")\n",
    "print(train_data[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量：496 测试集数量：123\n",
      "实例：\n",
      "(['外', '观', '上', '人', '性', '化', '设', '计', '也', '有', '值', '得', '一', '提', '的', '细', '微', '之', '处'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], [-1, -1, -1, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "execution_count": 624
  },
  {
   "cell_type": "raw",
   "id": "af48bbb8a58fbf5a",
   "metadata": {},
   "source": [
    "1.2 将读入的数据包装为InputExample的形式\n",
    "这种写法是沿用BERT的代码。在Example中的属性包括："
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7ac84fcc6771484",
   "metadata": {},
   "source": [
    "guid：生成的唯一id\n",
    "text_a: 输入的句子\n",
    "text_b: 句子中的实体\n",
    "sentence_label: 句子的标注，即text_a的标注，其中实体对应标注为B-ASP和I-ASP，非实体的token标注为O\n",
    "aspect_label: 实体标注，对text_b的实体序列标注\n",
    "polarity: 情感标注，text_a中每个token的情感倾向性标注"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6ad66c1a1b63e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:00.686548Z",
     "start_time": "2024-06-09T05:29:00.680485Z"
    }
   },
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, text_a, text_b=None, sentence_label=None, aspect_label=None,\n",
    "                 polarity=None):  #InputExample类的构造函数\n",
    "        self.guid = guid  # 输入数据的id\n",
    "        self.text_a = text_a  # 输入的句子\n",
    "        self.text_b = text_b  # 句子中的aspect(名词)\n",
    "        self.sentence_label = sentence_label  # 句子标注\n",
    "        self.aspect_label = aspect_label  # 名词的标注\n",
    "        self.polarity = polarity  # 情感倾向\n",
    "\n",
    "\n",
    "def create_example(lines, set_type):\n",
    "    examples = []\n",
    "    for i, (sentence, tag, polarity) in enumerate(lines):\n",
    "        aspect = []\n",
    "        aspect_tag = []\n",
    "        aspect_polarity = [-1]\n",
    "        for w, t, p in zip(sentence, tag, polarity):\n",
    "            if t == \"B-ASP\" or t == \"I-ASP\":\n",
    "                aspect.append(w)\n",
    "                aspect_tag.append(t)\n",
    "                aspect_polarity.append(p)\n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        text_a = sentence\n",
    "        text_b = aspect\n",
    "        polarity.extend(aspect_polarity)  #为了避免在下面句子与句子间添加【sep】符号时报错\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, sentence_label=tag,\n",
    "                         aspect_label=aspect_tag, polarity=polarity))\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_examples = create_example(train_data, \"train\")\n",
    "test_examples = create_example(test_data, \"test\")\n",
    "print(train_examples[0].guid)\n",
    "print(train_examples[0].text_a)\n",
    "print(train_examples[0].text_b)\n",
    "print(train_examples[0].sentence_label)\n",
    "print(train_examples[0].aspect_label)\n",
    "print(train_examples[0].polarity)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-0\n",
      "['外', '观', '上', '人', '性', '化', '设', '计', '也', '有', '值', '得', '一', '提', '的', '细', '微', '之', '处']\n",
      "['设', '计']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['B-ASP', 'I-ASP']\n",
      "[-1, -1, -1, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 2]\n"
     ]
    }
   ],
   "execution_count": 625
  },
  {
   "cell_type": "raw",
   "id": "8315d1a3528629d2",
   "metadata": {},
   "source": [
    "1.3.1 一些常量\n",
    "MAX_SEQUENCE_LENGTH： 最大句子长度\n",
    "LABEL_LIST：实体标签\n",
    "PRETRAINED_BERT_MODEL：使用的预训练BERT模型\n",
    "NUM_LABELS：实体标签数，这里多出来的一个标签是为padding出来的位置准备的"
   ]
  },
  {
   "cell_type": "code",
   "id": "be70144259316fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:00.689612Z",
     "start_time": "2024-06-09T05:29:00.687474Z"
    }
   },
   "source": [
    "MAX_SEQUENCE_LENGTH = 80\n",
    "LABEL_LIST = [\"O\", \"B-ASP\", \"I-ASP\", \"[CLS]\", \"[SEP]\"]\n",
    "PRETRAINED_BERT_MODEL = \"bert-base-chinese\"\n",
    "NUM_LABELS = len(LABEL_LIST) + 1\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 32\n",
    "# DEVICE = \"cpu\"#\n",
    "DEVICE = \"mps\""
   ],
   "outputs": [],
   "execution_count": 626
  },
  {
   "cell_type": "raw",
   "id": "bbc4b5de7c50910e",
   "metadata": {},
   "source": [
    "1.3.2 从预训练的模型中加载tokenizer\n",
    "这里会自动从huggingface的网站上下载预训练模型的词典，tokenizer会完成WordPiece的分词工作。"
   ]
  },
  {
   "cell_type": "code",
   "id": "82abcc65604ccfec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:01.203202Z",
     "start_time": "2024-06-09T05:29:00.690881Z"
    }
   },
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_MODEL, do_lower_case=True)"
   ],
   "outputs": [],
   "execution_count": 627
  },
  {
   "cell_type": "markdown",
   "id": "ac6181a327f61c45",
   "metadata": {},
   "source": [
    "1.3.3 将原来数据中的情感标签对齐（把数据集里的2变为1）\n",
    "-1 negative 0 普通 1（2）positive"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5c7cdf0a9eab077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:01.211027Z",
     "start_time": "2024-06-09T05:29:01.206980Z"
    }
   },
   "source": [
    "def convert_polarity(examples):\n",
    "    for i in range(len(examples)):\n",
    "        polarities = []\n",
    "        for polarity in examples[i].polarity:\n",
    "            if polarity == 2:\n",
    "                polarities.append(1)\n",
    "            else:\n",
    "                polarities.append(polarity)\n",
    "        examples[i].polarity = polarities\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_examples = convert_polarity(train_examples)\n",
    "print(train_examples[0].polarity)\n",
    "test_examples = convert_polarity(test_examples)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 628
  },
  {
   "cell_type": "raw",
   "id": "a4764fdccfece9b6",
   "metadata": {},
   "source": [
    "1.3.4 将Example转化为输入特征\n",
    "InputFeatures类作为神经网络的输入，在转换时将所有属性填充到MAX_SEQUENCE_LENGTH。按照BERT的输入格式，text_a和text_b之间通过'[SEP]'符号连接\n",
    "其中的属性包括：\n",
    "input_ids_spc: 编码后的输入文本\n",
    "input_mask: 为1的位置表示真实token，为0表示填充占位符\n",
    "segment_ids: 为0表示单句，为1表示句子对\n",
    "label_id: 实体标注序列\n",
    "valid 指示哪些是有效的，1为真实\n",
    "label_mask: 1是真实标注，为0的位置表示填充占位符的标注\n",
    "polarities: 情感标注序列"
   ]
  },
  {
   "cell_type": "code",
   "id": "39329f9143808ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:01.213927Z",
     "start_time": "2024-06-09T05:29:01.211812Z"
    }
   },
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids_spc, input_mask, segment_ids, label_id,\n",
    "                 polarities=None, valid_ids=None, label_mask=None):\n",
    "        self.input_ids_spc = input_ids_spc\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\n",
    "        self.polarities = polarities"
   ],
   "outputs": [],
   "execution_count": 629
  },
  {
   "cell_type": "code",
   "id": "c0e625efd0eb451d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:01.222790Z",
     "start_time": "2024-06-09T05:29:01.215427Z"
    }
   },
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    label_map = {}\n",
    "    for i, label in enumerate(label_list, 1):\n",
    "        label_map[label] = i  #把每一种标志对应单独ID\n",
    "\n",
    "    features = []\n",
    "    for example in tqdm.tqdm(examples):\n",
    "        text_spc_tokens = example.text_a  #句子token\n",
    "        aspect_tokens = example.text_b  #名词token\n",
    "        sentence_label = example.sentence_label  #句子标注\n",
    "        aspect_label = example.aspect_label  #名词标注\n",
    "        polaritiylist = example.polarity  #情感\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        polarities = []\n",
    "        valid = []\n",
    "        label_mask = []\n",
    "        text_spc_tokens.extend(['[SEP]'])\n",
    "        text_spc_tokens.extend(aspect_tokens)  # 将输入文本（text_a）和识别出来的实体(text_b)连接起来\n",
    "        sentence_label.extend(['[SEP]'])\n",
    "        sentence_label.extend(aspect_label)  #将句子标注和名词标注结合\n",
    "        label_lists = sentence_label  #标注列表\n",
    "        enum_tokens = text_spc_tokens  #所有原文本和名词文本\n",
    "        for i, word in enumerate(enum_tokens):  # 为文本和实体生成标签序列\n",
    "            token = tokenizer.tokenize(word)  #分词\n",
    "            tokens.extend(token)\n",
    "            label_1 = label_lists[i]  #添加标签\n",
    "            polarity_1 = polaritiylist[i]  #添加情感\n",
    "            for m in range(len(token)):  # 一个词，只在首字上标注\n",
    "                if m == 0:\n",
    "                    labels.append(label_1)\n",
    "                    polarities.append(polarity_1)\n",
    "                    valid.append(1)\n",
    "                    label_mask.append(1)\n",
    "                else:\n",
    "                    valid.append(0)\n",
    "        #如果超出max_seq_length则截断，留一个位置给【sep】\n",
    "        if len(tokens) >= max_seq_length - 1:\n",
    "            #token 从开始到max_seq_length-2 留两个位置给sep和cls标志符\n",
    "            tokens = tokens[0:(max_seq_length - 2)]\n",
    "            polarities = polarities[0:(max_seq_length - 2)]\n",
    "            labels = labels[0:(max_seq_length - 2)]\n",
    "            valid = valid[0:(max_seq_length - 2)]\n",
    "            label_mask = label_mask[0:(max_seq_length - 2)]\n",
    "        segment_ids = []  #单句子时为0，句子对时为1\n",
    "        label_ids = []  #标签id\n",
    "        final_tokens = []  #最终token（加入标志符后的）\n",
    "\n",
    "        #添加CLS标记\n",
    "        final_tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.insert(0, 1)\n",
    "        label_mask.insert(0, 1)\n",
    "        label_ids.append(label_map[\"[CLS]\"])  #插入cls对应的唯一ID\n",
    "        #添加token\n",
    "        for i, token in enumerate(tokens):\n",
    "            final_tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        #添加SEP标记\n",
    "        final_tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.append(1)\n",
    "        label_mask.append(1)\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "\n",
    "        input_ids_spc = tokenizer.convert_tokens_to_ids(final_tokens)  #把token转换成id（使用内置词典）\n",
    "        input_mask = [1] * len(input_ids_spc)\n",
    "        label_mask = [1] * len(label_ids)\n",
    "        # 将各属性补齐\n",
    "        while len(input_ids_spc) < max_seq_length:\n",
    "            input_ids_spc.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(0)\n",
    "            valid.append(1)\n",
    "            label_mask.append(0)\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(0)\n",
    "        while len(polarities) < max_seq_length:\n",
    "            polarities.append(-1)\n",
    "        assert len(input_ids_spc) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids_spc=input_ids_spc,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_ids,\n",
    "                          polarities=polarities,\n",
    "                          valid_ids=valid,\n",
    "                          label_mask=label_mask))\n",
    "    return features"
   ],
   "outputs": [],
   "execution_count": 630
  },
  {
   "cell_type": "code",
   "id": "a7bdcdca5eb043b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:01.448332Z",
     "start_time": "2024-06-09T05:29:01.223559Z"
    }
   },
   "source": [
    "train_features = convert_examples_to_features(train_examples, LABEL_LIST, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_features(test_examples, LABEL_LIST, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "print(train_features[0].input_ids_spc)\n",
    "print(train_features[0].input_mask)\n",
    "print(train_features[0].segment_ids)\n",
    "print(train_features[0].label_id)\n",
    "print(train_features[0].valid_ids)\n",
    "print(train_features[0].label_mask)\n",
    "print(train_features[0].polarities)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [00:00<00:00, 2639.12it/s]\n",
      "100%|██████████| 123/123 [00:00<00:00, 4027.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1912, 6225, 677, 782, 2595, 1265, 6392, 6369, 738, 3300, 966, 2533, 671, 2990, 4638, 5301, 2544, 722, 1905, 102, 6392, 6369, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[4, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 631
  },
  {
   "cell_type": "raw",
   "id": "126c11466aeb7417",
   "metadata": {},
   "source": [
    "2.构造模型\n",
    "在本教程中我们使用pytorch深度学习框架和基于Pytorch实现的Pytorch_transformer模块，这一模块实现了BERT为代表的多种transformer模型，并将预训练语言模型集成到模块中，通过一行代码就可以方便地下载并加载预训练模型。\n",
    "（1）加载预训练的BERT模型\n",
    "（2）定义网络结构\n",
    "（3）定义优化器\n",
    "（4）定义输入"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b59ee241dc55537",
   "metadata": {},
   "source": [
    "2.1 加载预训练的BERT模型\n",
    "这里会自动从huggingface的网站上下载预训练的BERT模型，在下载之后需要重新设置类别数量"
   ]
  },
  {
   "cell_type": "code",
   "id": "514ceec77a1ba731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:02.502493Z",
     "start_time": "2024-06-09T05:29:01.449056Z"
    }
   },
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_base_model = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n",
    "bert_base_model.config.num_labels = NUM_LABELS"
   ],
   "outputs": [],
   "execution_count": 632
  },
  {
   "cell_type": "raw",
   "id": "4b73ab1231ca9dd5",
   "metadata": {},
   "source": [
    "2.2.1 定义自注意力机制\n",
    "先在BERT的基础上实现一个新的自注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "id": "8ec63cc3f2710268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:02.506492Z",
     "start_time": "2024-06-09T05:29:02.503441Z"
    }
   },
   "source": [
    "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
    "\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.config = config\n",
    "        self.SA = BertSelfAttention(config)  #实现了官方的自注意力机制\n",
    "        self.tanh = torch.nn.Tanh()  #使用Tanh作为激活函数\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_vec = np.zeros((inputs.size(0), 1, 1, MAX_SEQUENCE_LENGTH))\n",
    "        zero_tensor = torch.tensor(zero_vec).float().to(DEVICE)\n",
    "        SA_out = self.SA(inputs, zero_tensor)\n",
    "        return self.tanh(SA_out[0])"
   ],
   "outputs": [],
   "execution_count": 633
  },
  {
   "cell_type": "raw",
   "id": "8bad860f6cc41750",
   "metadata": {},
   "source": [
    "2.2.2 Bert模型网络结构\n",
    "在Pytorch中，可以通过定义一个Module类的子类，并覆盖其forward方法。\n",
    "pytorch_transformer中的BertForTokenClassification类是Module的子类，\n",
    "我们继承BertForTokenClassification类，覆盖里面的forward方法。forward方法定义了神经网络前向传播的过程。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:05.011455Z",
     "start_time": "2024-06-09T05:29:02.507320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "class ModelBert(BertForTokenClassification):\n",
    "    def __init__(self, bert_base_model):\n",
    "        config = bert_base_model.config\n",
    "        super(ModelBert, self).__init__(config=config)\n",
    "        #定义基础bert任务\n",
    "        self.bert_for_global_context = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n",
    "        #池化层，对BERT模型的输出进行处理，从而得到一个固定长度的表示\n",
    "        self.pooler = BertPooler(config)\n",
    "        #将768维的向量映射到3维，用于最终的分类任务。\n",
    "        self.dense = torch.nn.Linear(768, 3)\n",
    "        #随机将一些神经元的输出设为0，以防止过拟合。\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        #两个自定义的自注意力层，用于处理输入的注意力机制。\n",
    "        self.SA1 = SelfAttention(config)\n",
    "        self.SA2 = SelfAttention(config)\n",
    "        #分别将向量映射回768维。\n",
    "        self.linear_double = torch.nn.Linear(768 * 2, 768)\n",
    "        self.linear_triple = torch.nn.Linear(768 * 3, 768)\n",
    "\n",
    "    #在BERT模型中，[SEP] 标记用于分隔不同句子。为了只关注 SEP 标记之前的部分，这个方法将 SEP 标记后的所有标记设为0。\n",
    "    def get_ids_for_local_context_extractor(self, text_indices):  #传入一个Tensor张量，里面内容是embedding后的文本\n",
    "        text_ids = text_indices.detach().cpu().numpy()  #把张量detach，不计算梯度，并且转换为numpy数组\n",
    "        for text_i in range(len(text_ids)):\n",
    "            sep_index = np.argmax((text_ids[text_i] == 102))  #检查<第一个>Sep标志的位置\n",
    "            text_ids[text_i][sep_index + 1:] = 0  #将后续全部置为0\n",
    "        return torch.tensor(text_ids).to(DEVICE)  #转换为张量并移动到device\n",
    "\n",
    "    #同样的，label也经过上述处理\n",
    "    def get_batch_token_labels_bert_base_indices(self, labels):\n",
    "        if labels is None:\n",
    "            return\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        for text_i in range(len(labels)):\n",
    "            sep_index = np.argmax((labels[text_i] == 5))\n",
    "            labels[text_i][sep_index + 1:] = 0\n",
    "        return torch.tensor(labels).to(DEVICE)\n",
    "\n",
    "    def get_batch_polarities(self, b_polarities):\n",
    "        b_polarities = b_polarities.detach().cpu().numpy()\n",
    "        shape = b_polarities.shape  #获取b_polarities的形状（纬度）\n",
    "        polarities = np.zeros((shape[0]))  #创建一个零向量\n",
    "        i = 0\n",
    "        for polarity in b_polarities:\n",
    "            polarity_idx = np.flatnonzero(polarity + 1)  #把polarity+1，再找出非零元素的位置\n",
    "            polarities[i] = polarity[polarity_idx[0]]  #取出第一个非零元素，并将其作为该输入的单一极性标签，存储在 polarities[i] 中。\n",
    "            i += 1\n",
    "        polarities = torch.from_numpy(polarities).long().to(DEVICE)  #数组转换回 PyTorch 张量,并移动回device\n",
    "        return polarities\n",
    "\n",
    "    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarities=None,\n",
    "                valid_ids=None, attention_mask_label=None):\n",
    "        #input_ids_spc: 编码后的输入文本\n",
    "        #label_id: 实体标注序列\n",
    "        #valid 指示哪些是有效的，1为真实\n",
    "        #label_mask: 1是真实标注，为0的位置表示填充占位符的标注\n",
    "        #polarities: 情感标注序列\n",
    "        input_ids_spc = self.get_ids_for_local_context_extractor(input_ids_spc)\n",
    "        labels = self.get_batch_token_labels_bert_base_indices(labels)\n",
    "        polarity_labels = self.get_batch_polarities(polarities)\n",
    "\n",
    "        global_context_out = self.bert_for_global_context(input_ids_spc, attention_mask=attention_mask)[\n",
    "            0]  #调用默认的bert预训练模型，输出包含序列1中每个标记的隐藏状态向量\n",
    "        batch_size, sequence_length, hidden_size = global_context_out.shape  #global_context_out 的形状为 (batch_size, sequence_length, hidden_size)\n",
    "        global_valid_output = torch.zeros(batch_size, sequence_length, hidden_size, dtype=torch.float32).to(DEVICE)\n",
    "        #经过此步骤后，模型将更关注有效标记的隐藏状态向量\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(sequence_length):\n",
    "                if valid_ids[i][j].item() == 1:\n",
    "                    jj += 1\n",
    "                    global_valid_output[i][jj] = global_context_out[i][j]\n",
    "\n",
    "        global_context_out = self.dropout(global_valid_output)\n",
    "        pooled_out = self.pooler(global_context_out)\n",
    "        pooled_out = self.dropout(pooled_out)\n",
    "        ate_logits = self.classifier(global_context_out)  #对每个标记的隐藏状态进行分类，生成序列标注任务（如命名实体识别）的预测结果 ate_logits\n",
    "        apc_logits = self.dense(pooled_out)  #模型对极性分类任务的输出预测结果\n",
    "\n",
    "        if labels is not None:\n",
    "            #损失函数（交叉熵损失）\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "            loss_sen = torch.nn.CrossEntropyLoss()\n",
    "            #计算序列标注任务的损失\n",
    "            loss_ate = loss_fct(ate_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            #计算分类任务损失\n",
    "            loss_apc = loss_sen(apc_logits, polarity_labels)\n",
    "            return loss_ate, loss_apc\n",
    "        else:\n",
    "            return ate_logits, apc_logits\n",
    "\n",
    "\n",
    "model = ModelBert(bert_base_model)"
   ],
   "id": "f97a200c53ae4d77",
   "outputs": [],
   "execution_count": 634
  },
  {
   "cell_type": "raw",
   "id": "51ac62e096c9fdbf",
   "metadata": {},
   "source": [
    "2.2.3 将模型加载到对应的计算设备上\n",
    "与tensorflow不同，Pytorch需要我们通过函数调用完成模型加载到设备上的过程。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:07.754331Z",
     "start_time": "2024-06-09T05:29:05.012297Z"
    }
   },
   "cell_type": "code",
   "source": "_ = model.to(DEVICE)",
   "id": "3bb18cf9c7b8aa55",
   "outputs": [],
   "execution_count": 635
  },
  {
   "cell_type": "raw",
   "id": "87a49abbe3bf4be8",
   "metadata": {},
   "source": [
    "2.3 设置优化器\n",
    "与其他教程相似，在训练中我们使用学习率衰减的策略"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad49195d52c46f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:07.764529Z",
     "start_time": "2024-06-09T05:29:07.757064Z"
    }
   },
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "param_optimizer = list(model.named_parameters())  # 模型中的所有参数\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']  #定义不进行权重衰减的参数\n",
    "\n",
    "#将模型参数分为两组：\n",
    "#第一组参数:\n",
    "#params: 过滤出不包含 no_decay 中指定参数的所有参数。\n",
    "#weight_decay: 设置权重衰减率为 0.00001。\n",
    "\n",
    "#第二组参数:\n",
    "#params: 过滤出包含 no_decay 中指定参数的所有参数。\n",
    "#weight_decay: 设置权重衰减率为 0.00001（尽管设置了 weight_decay，但实际不会应用到这些参数上）。\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.00001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, weight_decay=0.00001)"
   ],
   "outputs": [],
   "execution_count": 636
  },
  {
   "cell_type": "raw",
   "id": "9d47ac0c1e77f160",
   "metadata": {},
   "source": [
    "2.4 设置输入\n",
    "构造DataLoader，首先将InputFeature中的特征转化为pytorch中的Tensor，然后生成TensorDataset和SequentialSampler，再将dataset和sampler结合成为DataLoader，为之后模型训练和测试提供数据。\n",
    "DataLoader是一个可迭代对象，在训练和测试时可以每次返回一个batch的数据，并且可以利用多进程进行加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1d0acc89da28e",
   "metadata": {},
   "source": [
    "2.4.1 设置训练的输入"
   ]
  },
  {
   "cell_type": "code",
   "id": "7011d75ff85a97d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:07.782480Z",
     "start_time": "2024-06-09T05:29:07.765134Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
    "\n",
    "all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\n",
    "all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\n",
    "all_polarities = torch.tensor([f.polarities for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_spc_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_polarities,\n",
    "                           all_valid_ids, all_lmask_ids)  #组合为数据集\n",
    "train_sampler = SequentialSampler(train_data)  #顺序采样数据\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)  #分批加载数据"
   ],
   "outputs": [],
   "execution_count": 637
  },
  {
   "cell_type": "markdown",
   "id": "16cbcfe3767bd86a",
   "metadata": {},
   "source": [
    "2.4.2 设置测试的输入"
   ]
  },
  {
   "cell_type": "code",
   "id": "54a74fe44efe551a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:07.790740Z",
     "start_time": "2024-06-09T05:29:07.783457Z"
    }
   },
   "source": [
    "all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_polarities = torch.tensor([f.polarities for f in test_features], dtype=torch.long)\n",
    "all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)\n",
    "all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_spc_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_polarities,\n",
    "                          all_valid_ids, all_lmask_ids)\n",
    "eval_sampler = RandomSampler(eval_data)  #随机加载数据\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": 638
  },
  {
   "cell_type": "raw",
   "id": "a405d83aea19350a",
   "metadata": {},
   "source": [
    "3.训练模型\n",
    "（1）设置模型训练过程中的超参数，初始化日志\n",
    "（2）定义评估函数\n",
    "（3）模型训练并监督训练过程\n",
    "（4）保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad770eadfdd7b3",
   "metadata": {},
   "source": [
    "3.1 设置训练过程中的超参数，并利用logging模块输出训练过程的日志"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8c643b7d26768a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:07.793759Z",
     "start_time": "2024-06-09T05:29:07.791527Z"
    }
   },
   "source": [
    "import sys\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "EPOCH = 5  # 共计算5个epoch\n",
    "EVAL_STEP = 10  # 每10个step执行一个评估\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ],
   "outputs": [],
   "execution_count": 639
  },
  {
   "cell_type": "raw",
   "id": "d85ebe092e175758",
   "metadata": {},
   "source": [
    "3.2 定义评估函数\n",
    "这里我们用scikit-learn中计算的F1值作为评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b38f3689fbda71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:07.801621Z",
     "start_time": "2024-06-09T05:29:07.794439Z"
    }
   },
   "source": [
    "def evaluate(dataloader, label_list):\n",
    "    apc_result = {'max_apc_test_acc': 0, 'max_apc_test_f1': 0}\n",
    "    ate_result = 0\n",
    "    y_true = []  #真实标签\n",
    "    y_pred = []  #预测标签\n",
    "    n_test_correct, n_test_total = 0, 0  #用于定义极性分类的准确率\n",
    "    test_apc_logits_all, test_polarities_all = None, None  #用于存储所有的预测和真实极性标签。\n",
    "    model.eval()  # 将网络设置为评估的状态\n",
    "    label_map = {i: label for i, label in enumerate(label_list, 1)}\n",
    "    for input_ids_spc, input_mask, segment_ids, label_ids, polarities, valid_ids, l_mask in dataloader:  #遍历数据加载器中的批次数据。\n",
    "        input_ids_spc = input_ids_spc.to(DEVICE)\n",
    "        input_mask = input_mask.to(DEVICE)\n",
    "        segment_ids = segment_ids.to(DEVICE)\n",
    "        valid_ids = valid_ids.to(DEVICE)\n",
    "        label_ids = label_ids.to(DEVICE)\n",
    "        polarities = polarities.to(DEVICE)\n",
    "        l_mask = l_mask.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            ate_logits, apc_logits = model(\n",
    "                input_ids_spc, segment_ids, input_mask,\n",
    "                valid_ids=valid_ids, polarities=polarities, attention_mask_label=l_mask)  #计算损失\n",
    "        polarities = model.get_batch_polarities(polarities)\n",
    "        #逐元素比较预测的类别索引和实际的类别索引，如果相等则返回 True，否则返回 False，结果是一个布尔类型的张量，计算为true的个数，放到n_test_correct\n",
    "        n_test_correct += (torch.argmax(apc_logits, -1) == polarities).sum().item()\n",
    "        n_test_total += len(polarities)\n",
    "\n",
    "        if test_polarities_all is None:\n",
    "            test_polarities_all = polarities\n",
    "            test_apc_logits_all = apc_logits\n",
    "        else:\n",
    "            test_polarities_all = torch.cat((test_polarities_all, polarities), dim=0)\n",
    "            test_apc_logits_all = torch.cat((test_apc_logits_all, apc_logits), dim=0)\n",
    "            label_ids = model.get_batch_token_labels_bert_base_indices(label_ids)  #处理标签\n",
    "        ate_logits = torch.argmax(F.log_softmax(ate_logits, dim=2), dim=2)\n",
    "        ate_logits = ate_logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        input_mask = input_mask.to('cpu').numpy()\n",
    "        for i, label in enumerate(label_ids):\n",
    "            temp_1 = []\n",
    "            temp_2 = []\n",
    "            for j, m in enumerate(label):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                elif label_ids[i][j] == len(\n",
    "                        label_list):  #标签等于标签列表的长度（表示结束），将临时列表 temp_1 和 temp_2 中的内容添加到 y_true 和 y_pred。\n",
    "                    y_true += temp_1\n",
    "                    y_pred += temp_2\n",
    "                    break\n",
    "                else:  #否则，将标签和预测结果添加到临时列表中。\n",
    "                    temp_1.append(label_map.get(label_ids[i][j], 'O'))\n",
    "                    temp_2.append(label_map.get(ate_logits[i][j], 'O'))\n",
    "                    test_acc = n_test_correct / n_test_total  #计算极性分类任务的准确率 test_acc。\n",
    "    test_f1 = f1_score(torch.argmax(test_apc_logits_all, -1).cpu(), test_polarities_all.cpu(), labels=[0, 1],\n",
    "                       average='macro')\n",
    "    test_acc = round(test_acc * 100, 2)\n",
    "    test_f1 = round(test_f1 * 100, 2)\n",
    "    apc_result = {'max_apc_test_acc': test_acc, 'max_apc_test_f1': test_f1}\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    tmps = report.split()\n",
    "    ate_result = round(float(tmps[7]) * 100, 2)\n",
    "    return apc_result, ate_result"
   ],
   "outputs": [],
   "execution_count": 640
  },
  {
   "cell_type": "raw",
   "id": "e0bae439a0164716",
   "metadata": {},
   "source": [
    "3.3 训练模型\n",
    "在每个EPOCH中，每次输入一个batch的数据，计算损失和损失反向传播完成一个step的训练。每经过EVAL_STEP个step做一次评估，并记录下训练过程中最好的评价指标\n",
    "TIPS: 在前向传播之前要把模型的模式设为训练模式（第10行），这会把网络中的Dropout和BatchNormilzation关掉。"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ca2d8538e043ce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:29:58.969063Z",
     "start_time": "2024-06-09T05:29:07.802423Z"
    }
   },
   "source": [
    "max_apc_test_acc = 0\n",
    "max_apc_test_f1 = 0\n",
    "max_ate_test_f1 = 0\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # 每个epoch\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 一个step\n",
    "        model.train()  # 将网络设置为train的模式\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids_spc, input_mask, segment_ids, label_ids, polarities, valid_ids, l_mask = batch  # 取一个batch的数据\n",
    "\n",
    "        loss_ate, loss_apc = model(\n",
    "            input_ids_spc, segment_ids, input_mask, label_ids, polarities, valid_ids, l_mask)  # 前向传播，计算损失\n",
    "        loss = loss_ate + loss_apc\n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "        nb_tr_examples += input_ids_spc.size(0)  #累加当前 batch 的样本数。\n",
    "        nb_tr_steps += 1  #增加训练步数计数器。\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1  #增加全局步数计数器。\n",
    "        if global_step % EVAL_STEP == 0:  # 评估\n",
    "            apc_result, ate_result = evaluate(eval_dataloader, LABEL_LIST)\n",
    "            if apc_result['max_apc_test_acc'] > max_apc_test_acc:\n",
    "                max_apc_test_acc = apc_result['max_apc_test_acc']\n",
    "            if apc_result['max_apc_test_f1'] > max_apc_test_f1:\n",
    "                max_apc_test_f1 = apc_result['max_apc_test_f1']\n",
    "            if ate_result > max_ate_test_f1:\n",
    "                max_ate_test_f1 = ate_result\n",
    "            current_apc_test_acc = apc_result['max_apc_test_acc']\n",
    "            current_apc_test_f1 = apc_result['max_apc_test_f1']\n",
    "            current_ate_test_f1 = round(ate_result, 2)\n",
    "    logger.info('Epoch %s' % epoch)\n",
    "    logger.info(f'APC_test_acc: {current_apc_test_acc}(max: {max_apc_test_acc})  '\n",
    "                f'APC_test_f1: {current_apc_test_f1}(max: {max_apc_test_f1})')\n",
    "    logger.info(f'ATE_test_f1: {current_ate_test_f1}(max:{max_ate_test_f1})')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "APC_test_acc: 82.93(max: 82.93)  APC_test_f1: 81.69(max: 81.69)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "ATE_test_f1: 57.95(max:57.95)\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "APC_test_acc: 88.62(max: 88.62)  APC_test_f1: 87.35(max: 87.35)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n",
      "ATE_test_f1: 89.8(max:89.8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[641], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m loss_ate, loss_apc \u001B[38;5;241m=\u001B[39m model(\n\u001B[1;32m     16\u001B[0m     input_ids_spc, segment_ids, input_mask, label_ids, polarities, valid_ids, l_mask)  \u001B[38;5;66;03m# 前向传播，计算损失\u001B[39;00m\n\u001B[1;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_ate \u001B[38;5;241m+\u001B[39m loss_apc\n\u001B[0;32m---> 18\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 反向传播计算梯度\u001B[39;00m\n\u001B[1;32m     19\u001B[0m nb_tr_examples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m input_ids_spc\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m#累加当前 batch 的样本数。\u001B[39;00m\n\u001B[1;32m     20\u001B[0m nb_tr_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m#增加训练步数计数器。\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/BigData/lib/python3.10/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/BigData/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/BigData/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 641
  },
  {
   "cell_type": "markdown",
   "id": "ba6afff717c6f199",
   "metadata": {},
   "source": [
    "3.4 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0419f1b97718788",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SAVE_PATH = \"./temp\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "label_map = {i: label for i, label in enumerate(LABEL_LIST, 1)}\n",
    "model_config = {\n",
    "    \"bert_model\": PRETRAINED_BERT_MODEL,\n",
    "    \"do_lower\": True,\n",
    "    \"max_seq_length\": MAX_SEQUENCE_LENGTH,\n",
    "    \"num_labels\": len(LABEL_LIST) + 1,\n",
    "    \"label_map\": label_map\n",
    "}\n",
    "json.dump(model_config, open(os.path.join(SAVE_PATH, \"config.json\"), \"w\"))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
